---
title: "615 final"
author: "Kailun Huang"
date: "12/11/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library('devtools')
library('twitteR')
library('stringr')
library('graphTweets')
library('igraph')
library('streamR')
library('ROAuth')
library('tm')
library('wordcloud')
library('tidytext')
library('dplyr')
library('reshape2')
library('ggplot2')
```
#### For code description, Please see my Rmd File. This is just the final report


## set up twitteR
```{r,echo=FALSE}
api_key <- 	"RIjMQnOXRA3hWjmLmNttEkhZn"
api_secret <- "CDVOPsKE6t6CFNqUKJdUeZ4fnFBGHGjRjjvTZ0t82YGGlGh5Xr"
access_token <- "927639920851681280-zJmoliQVXHXOT9GaoOsnvyaYD4f17G7"
access_token_secret <- "eTrCgqFfBqzuKz0rFxcFqwmLIHqrP9QZMsEhMfakiWfoc"

setup_twitter_oauth(api_key, 
                    api_secret, 
                    access_token, 
                    access_token_secret)
##requestURL <- "https://api.twitter.com/oauth/request_token"
##accessURL <- "https://api.twitter.com/oauth/access_token"
##authURL <- "https://api.twitter.com/oauth/authorize"
##consumerKey <- 	"LFNRqX5i1PkB69SjEEncXWloq"
##consumerSecret <- "4sDHqY6aLm7PRfJLxpq6GsWqphZxzX3dXLjssSLXYhO8wPwL3F"
##my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret, 
                            ## requestURL = requestURL, accessURL = accessURL, authURL = authURL)
##my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
##save(my_oauth, file = "my_oauth.Rdata")

```


```{r, echo=FALSE}
# get data from twitter
cp3<-searchTwitter('Chris Paul',since='2017-06-28', until='2017-12-12', n = 1000,lang="en")
# transfer data to a data frame
cp3.df <- twListToDF(cp3)
# data cleaning, remove punctuation, non-english symbol, remove numbers, remove url, remove non-english character
cp3.df$text = gsub("&amp", "", cp3.df$text)
cp3.df$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", cp3.df$text)
cp3.df$text = gsub("@\\w+", "",cp3.df$text)
cp3.df$text = gsub("[[:punct:]]", "", cp3.df$text)
cp3.df$text = gsub("[[:digit:]]", "", cp3.df$text)
cp3.df$text = gsub("http\\w+", "", cp3.df$text)
cp3.df$text = gsub("[ \t]{2,}", "", cp3.df$text)
cp3.df$text = gsub("^\\s+|\\s+$", "", cp3.df$text)
cp3.df$text <- str_replace_all(cp3.df$text," "," ")
cp3.df$text<-iconv(cp3.df$text, from = "latin1", to = "ASCII", sub="")
# transfer data fram to a csv file
write.csv(cp3.df,"cp3.csv")
```
### First of all, we want to collect data from R and search players information on tweeter. Then used 'gsub' function to clean the data. Finally save the data frame to a csv file.


```{r, echo=FALSE}
# read csv file
cp<-read.csv('cp3.csv')
# replace all the unknown symbol, tranform capital to lower letter, remove common words, remove stop words
cp3_corpus<-Corpus(VectorSource(str_replace_all(cp$text, "@", "")))
cp3_corpus<-tm_map(cp3_corpus, content_transformer(tolower))
cp3_corpus<-tm_map(cp3_corpus, removeWords, stopwords("english"))
cp3_corpus<-tm_map(cp3_corpus, removeWords, c("cp3","paul","chris"))
# create word cloud
wordcloud(cp3_corpus,random.order = F,max.words = 40,scale = c(3,0.5),colors = rainbow(50))
# save it as rds file
saveRDS(cp3_corpus,"cp3_corpus.rds")
```
### Then we need to read the csv file and make word cloud. The more a specific word appears, the bigger and bolder the word will appear in the visualization. The most frequent words with Chris Paul is rockets, james and harden. It is normal because he plays in rockets and his team mates is James Harden.The key words relate to James Harden are “lebron” and “mvp”. These words mean people compliment Harden performance.





```{r,echo=FALSE}
jh<-searchTwitter('James Harden',since='2017-06-28', until='2017-12-12', n = 1000,lang="en")
jh.df <- twListToDF(jh)
jh.df$text = gsub("&amp", "", jh.df$text)
jh.df$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", jh.df$text)
jh.df$text = gsub("@\\w+", "",jh.df$text)
jh.df$text = gsub("[[:punct:]]", "", jh.df$text)
jh.df$text = gsub("[[:digit:]]", "", jh.df$text)
jh.df$text = gsub("http\\w+", "", jh.df$text)
jh.df$text = gsub("[ \t]{2,}", "", jh.df$text)
jh.df$text = gsub("^\\s+|\\s+$", "", jh.df$text)
jh.df$text <- str_replace_all(jh.df$text," "," ")
jh.df$text<-iconv(jh.df$text, from = "latin1", to = "ASCII", sub="")
write.csv(jh.df,"jh.csv")
```

```{r,echo=FALSE}
jh<-read.csv('jh.csv')
jh_corpus<-Corpus(VectorSource(str_replace_all(jh$text, "@", "")))
jh_corpus<-tm_map(jh_corpus, content_transformer(tolower))
jh_corpus<-tm_map(jh_corpus, removeWords, stopwords("english"))
jh_corpus<-tm_map(jh_corpus, removeWords, c("james","harden"))
wordcloud(jh_corpus,random.order = F,max.words = 40,scale = c(3,0.5),colors = rainbow(50))

saveRDS(jh_corpus,"jh_corpus.rds")

```
### Sentiment Analysis
```{r,echo=FALSE}
## sentiment analysis for Chris Paul
cp3sent <- data.frame(lapply(cp, as.character), stringsAsFactors=FALSE)
## use select to filter data only contain text column and give each row a value X.
cp3_text_source <- select(cp3sent,text,X)
cp3_text <- melt(data = cp3_text_source, id.vars = "X")
cp3_text <- data_frame(line = cp3_text$X, text = cp3_text$value)
cp3_text <- cp3_text %>%
  unnest_tokens(word, text)
## clean usual words
data(stop_words)

cp3_text <- cp3_text %>%
  anti_join(stop_words) 

cp3_text %>%
  count(word, sort = TRUE) 
## filter out happiness words
nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

cp3_text_sentiment_stat <- cp3_text %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE)
## make the text line variable as number
cp3_text$line <- as.numeric(cp3_text$line)

bing_word_counts <- cp3_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
saveRDS(bing_word_counts,"bing_word_counts.rds")
```

```{r,echo=FALSE}
  # sentiment analysis for harden. It's the same code as Paul's

jhsent <- data.frame(lapply(jh, as.character), stringsAsFactors=FALSE)

jh_text_source <- select(jhsent,text,X)
jh_text <- melt(data = jh_text_source, id.vars = "X")
jh_text <- data_frame(line = jh_text$X, text = jh_text$value)
jh_text <- jh_text %>%
  unnest_tokens(word, text)

data(stop_words)

jh_text <- jh_text %>%
  anti_join(stop_words) 

jh_text %>%
  count(word, sort = TRUE) 

nrcjoy1 <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

jh_text_sentiment_stat <- jh_text %>%
  inner_join(nrcjoy1) %>%
  count(word, sort = TRUE)

jh_text$line <- as.numeric(jh_text$line)

bing_word_counts1 <- jh_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts1

bing_word_counts1 %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
saveRDS(bing_word_counts1,"bing_word_counts1.rds")
```
### Sentiment analysis used to determine the attitude of the text data looking at the number of positive and negative words. The most frequent negative word toward Chris Paul is harden.The most frequent positive word toward Chris Paul is compliment. The most frequent negative word toward Harden is harden.The most frequent positive word toward Harden is lead.





## ggmap for two players
```{r,echo=FALSE}
load("my_oauth.Rdata")

## using filter stream to collect location data. Twitter does not show location
filterStream("cp3.json", 
             track=c("CP3"), 
             locations = c(-125, 25, -66,50), 
             timeout=20, oauth=my_oauth)
cp<-parseTweets("cp3.json", verbose = TRUE)
ck1 <- sum(cp$lat>0, na.rm = TRUE)
ck2 <- sum(cp$place_lat>0, na.rm = TRUE)
ck3 <- sum(!is.na(cp$location))
## using map data to locate United State location
cpdata <- map_data("state")   
cppoints <- data.frame(x = as.numeric(cp$lon),  
                       y = as.numeric(cp$lat))
cppoints <- cppoints[cppoints$y > 25, ]  
cppoints<-filter(cppoints,y>19&y<65,x>(-161.7)&x<(-68.01))
## using ggplot to draw the map and show twitters location on map by points

ggplot(cpdata) + 
  geom_map(aes(map_id = region),  
           map =cpdata,  
           fill = "white",             
           color = "grey20", size = 0.25) + 
  expand_limits(x = cpdata$long, y = cpdata$lat) +            
  theme(axis.line = element_blank(),  
        axis.text = element_blank(),  
        axis.ticks = element_blank(),                     
        axis.title = element_blank(),  
        panel.background = element_blank(),  
        panel.border = element_blank(),                     
        panel.grid.major = element_blank(), 
        plot.background = element_blank(),                     
        plot.margin = unit(0 * c( -1.5, -1.5, -1.5, -1.5), "lines")) +  
        geom_point(data = cppoints,             
        aes(x = x, y = y), size = 1,  
        alpha = 1/5, color = "blue")
saveRDS(cppoints, file="cppoints.rds")
```

```{r,echo=FALSE}
load("my_oauth.Rdata")
## ggmap for tweets relate to James Harden

filterStream("jh1.json", 
             track=c("james"), 
             locations = c(-125, 25, -66,50), 
             timeout=20, oauth=my_oauth)
jh1<-parseTweets("jh1.json", verbose = TRUE)
ck1 <- sum(jh1$lat>0, na.rm = TRUE)
ck2 <- sum(jh1$place_lat>0, na.rm = TRUE)
ck3 <- sum(!is.na(jh1$location))

cpdata <- map_data("state")   
jhpoints <- data.frame(x = as.numeric(jh1$lon),  
                       y = as.numeric(jh1$lat))
jhpoints <- jhpoints[jhpoints$y > 25, ]  
jhpoints<-filter(jhpoints,y>19&y<65,x>(-161.7)&x<(-68.01))
ggplot(cpdata) + 
  geom_map(aes(map_id = region),  
           map =cpdata,  
           fill = "white",             
           color = "grey20", size = 0.25) + 
  expand_limits(x = cpdata$long, y = cpdata$lat) +            
  theme(axis.line = element_blank(),  
        axis.text = element_blank(),  
        axis.ticks = element_blank(),                     
        axis.title = element_blank(),  
        panel.background = element_blank(),  
        panel.border = element_blank(),                     
        panel.grid.major = element_blank(), 
        plot.background = element_blank(),                     
        plot.margin = unit(0 * c( -1.5, -1.5, -1.5, -1.5), "lines")) +  
        geom_point(data = jhpoints,             
        aes(x = x, y = y), size = 1,  
        alpha = 1/5, color = "red")
saveRDS(jhpoints, file="jhpoints.rds")


```
 ### By making ggplots of the United States, we can know where users from the states comment on these two players. Thus, we know which part in the country are interested in these two players.

